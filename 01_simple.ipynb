{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Fine tune Llama 2 7B on a single node with 8 GPUs\n","\n","### Docker setup\n","```bash\n","sudo docker run --gpus all -it --rm \\\n","\t-v $(pwd)/epfllm-megatron-llm/:/epfllm/ \\\n","  --workdir /epfllm \\\n","\t--shm-size=128gb \\\n","\t--ulimit memlock=-1 \\\n","\t--ulimit stack=67108864 \\\n"," \t--memory 480G \\\n","\tnvcr.io/nvidia/pytorch:23.07-py3\n","```\n","\n","install\n","```bash\n","cd Megatron-LLM\n","pip install -r requirements.txt\n","cd megatron/data/\n","make\n","cd ../../../\n","```\n","\n","login \n","`huggingface-cli login`\n","\n","\n","\n","### 1. Setup Environment \n","\n","```bash\n","# to install torch with the correct cuda version, check nvcc --version for apex\n","pip install torch --extra-index-url https://download.pytorch.org/whl/cu117 --upgrade\n","# regular install\n","pip install packaging ninja pybind11 \n","# apex (when not using the container)\n","git clone https://github.com/NVIDIA/apex\n","cd apex\n","python setup.py install --cuda_ext\n","cd ../\n","rm -rf apex\n","\n","# Megatron LLM\n","git clone https://github.com/epfLLM/Megatron-LLM.git\n","cd Megatron-LLM/\n","pip install -r requirements.txt\n","cd megatron/data/\n","make\n","cd ../../\n","```"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["%%bash\n","export CACHE_PATH=\"./cache\"\n","export DATASET_PATH=\"./dataset\"\n","export MEGATRON_PATH=\"./Megatron-LLM\"\n","export MODEL_PATH=\"./model\"\n","export MODEL_ID=\"meta-llama/Llama-2-7b-hf\""]},{"cell_type":"markdown","metadata":{},"source":["### 2. load dataset "]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["import os\n","import json\n","from datasets import load_dataset\n","\n","\n","# the `cache_dir` argument is optional\n","dataset = load_dataset(\"philschmid/wikipedia-230601-de-minhash-dedup\",\n","                       split=\"train\", cache_dir=CACHE_PATH)\n","\n","os.makedirs(DATASET_PATH, exist_ok=True)\n","with open(f\"{DATASET_PATH}/raw.jsonl\", \"w\") as f:\n","    for document in dataset:\n","        document = {\"id\": document[\"id\"], \"text\": document[\"text\"]}\n","        f.write(json.dumps(document) + \"\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Preprocess dataset \n","\n","We need the tokenizer"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"643478b18d9e4d4b80e50d66171ac712","version_major":2,"version_minor":0},"text/plain":["Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["'./model/tokenizer.model'"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["from huggingface_hub import hf_hub_download\n","\n","os.makedirs(MODEL_PATH, exist_ok=True)\n","\n","hf_hub_download(repo_id=MODEL_ID, filename=\"tokenizer.model\", repo_type=\"model\",local_dir=MODEL_PATH,local_dir_use_symlinks=False )"]},{"cell_type":"markdown","metadata":{},"source":["process the dataset, \n","\n","check out https://github.com/LAION-AI/Open-Assistant/tree/main/model/pretokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python {MEGATRON_PATH}/tools/preprocess_data.py --input={DATASET_PATH}/raw.jsonl \\\n","\t--output_prefix={DATASET_PATH}/megatron \\\n","\t--tokenizer_type=SentencePieceTokenizer \\\n","\t--vocab_file={MODEL_PATH}/tokenizer.model \\\n","\t--chunk_size=32 \\\n","\t--workers=96 \\\n","\t--append_eod \\\n","  --log_interval 10000 \\\n","\t--no_new_tokens \n"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Weight conversion"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/admin/home/philipp/micromamba/envs/hf/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n","[2023-09-22 15:28:28,110] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","Getting llama...\n","Weights at cache do not look like a meta checkpoint, assuming huggingface cache_dir instead\n","Loading checkpoint shards: 100%|██████████████████| 2/2 [00:44<00:00, 22.01s/it]\n","Converting weights: 100%|███████████████████████| 32/32 [00:03<00:00,  8.86it/s]\n","Saved weights in model\n","Saved tokenizer.model in model/tokenizer.model\n","Done\n"]}],"source":["%%bash\n","python ${MEGATRON_PATH}/weights_conversion/hf_to_megatron.py llama2 --size=7 --model-path ${MODEL_ID} \\\n","\t--out=${MODEL_PATH} --cache-dir=${CACHE_PATH}"]},{"cell_type":"markdown","metadata":{},"source":["validate model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%bash\n","# arguments required by `torchrun`\n","DISTRIBUTED_ARGS=\"--nproc_per_node 1 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 8000\"\n","LLAMA_ARGS=\"--use_rms_norm --glu_activation swiglu --no_tie_embed_logits --no_new_tokens --layernorm_epsilon 1e-5\"\n","COMMON_ARGS=\"--hidden_dropout 0.0 --attention_dropout 0.0 --no_bias_gelu_fusion\"\n","torchrun $DISTRIBUTED_ARGS ${MEGATRON_PATH}/verify_correctness.py \\\n","\t--model_name=llama2 \\\n","\t--model_size=7 \\\n","\t--load=${MODEL_PATH} \\\n","\t--data_path=${DATASET_PATH}/ \\\n","\t--tokenizer_type=SentencePieceTokenizer \\\n","\t--vocab_file=${MODEL_PATH}/tokenizer.model \\\n","\t--huggingface_cache=${CACHE_PATH} \\\n","\t--huggingface_device=cuda:1 \\\n","\t$COMMON_ARGS $LLAMA_ARGS "]},{"cell_type":"markdown","metadata":{},"source":["## 3. Shard model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%bash\n","python ${MEGATRON_PATH}/tools/checkpoint_util.py \\\n","\t--target_tensor_parallel_size 4 \\\n","\t--target_pipeline_parallel_size 1 \\\n","\t--load_dir ${MODEL_PATH} \\\n","\t--save_dir ${MODEL_PATH}_sharded \\\n","\t--model_type llama2 \\\n","\t--true_vocab_size 32000 \\\n","\t--bf16"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Train"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%bash\n","LOG_ARGS=\"--log_interval 1 --save_interval 100 --eval_interval 50\"\n","TRAIN_ARGS=\"--train_iters 500 --lr_decay_style cosine --lr_warmup_iters 50 --lr 3e-4 --min_lr 1e-6\"\n","DISTRIBUTED_ARGS=\"--nproc_per_node 8 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 8000\"\n","torchrun $DISTRIBUTED_ARGS ${MEGATRON_PATH}/finetune.py \\\n","\t--tensor_model_parallel_size 4 \\\n","\t--pipeline_model_parallel_size 1 \\\n","\t--load ${MODEL_PATH}_sharded \\\n","\t--save ${MODEL_PATH}_sharded \\\n","\t--tensorboard_dir ${MODEL_PATH}_sharded \\\n","\t--data_path ${DATASET_PATH}/megatron_text_document \\\n","\t--model_name llama2 \\\n","\t--tokenizer_type SentencePieceTokenizer \\\n","\t--vocab_file=${MODEL_PATH}/tokenizer.model \\\n","\t--bf16 \\\n","\t--use_flash_attn \\\n","\t--micro_batch_size 5 \\\n","\t--global_batch_size 1000 \\\n","\t--sequence_parallel \\\n","\t--recompute_granularity selective \\\n","\t--use_checkpoint_args \\\n","\t$COMMON_ARGS $LOG_ARGS $TRAIN_ARGS $LLAMA_ARGS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
