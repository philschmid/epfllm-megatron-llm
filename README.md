# Experiments and Samples for working with Megatron-LLM by epfLLM

Repository: https://github.com/epfLLM/Megatron-LLM
Documentation: https://epfllm.github.io/Megatron-LLM/index.html
FAQ: https://epfllm.github.io/Megatron-LLM/guide/faq.html

> This library enables pre-training and fine-tuning of large language models (LLMs) at scale. Our repository is a modification of the original Megatron-LM codebase by Nvidia. Added key features include:
> * Llama, Llama 2, Code Llama and Falcon support
> ...
> * RoPE scaling for longer attention context support
> * FlashAttention 2
> * BF16 / FP16 training
> Conversion to and from Hugging Face hub


# Example 

* [01_simple](./01_simple.ipynb)